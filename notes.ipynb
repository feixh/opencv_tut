{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV I/O functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above piece of code imported all the packages we need through this tutorial.\n",
    "\n",
    "Now, let's load an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, display and save images\n",
    "Use the function [`cv2.imread()`](http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html?highlight=imread#cv2.imread) to read an image. The function accepts two arguments, the first argument is the path of the image and the second argument is a flag indicating how to decode the image, there are three options:\n",
    "- `cv2.IMREAD_COLOR`: load the image in color mode\n",
    "- `cv2.IMREAD_GRAYSCALE`: load the image in grayscale mode\n",
    "- `cv2.IMREAD_UNCHANGED`: load the image in color+alpha mode if the image has an alpha (controls transparency) channel, otherwise in color mode\n",
    "\n",
    "Use the function [`cv2.imshow()`](http://docs.opencv.org/3.0-beta/modules/highgui/doc/user_interface.html?highlight=imshow#cv2.imshow) to display an image. This function accepts two arguments, the first argument is the window name and the second one is the image, which is an numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the image\n",
    "image = cv2.imread('dog.png', cv2.IMREAD_COLOR)\n",
    "# TODO: check size of images loaded with different options\n",
    "print image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display the image\n",
    "cv2.imshow('display', image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above doesn't show us the image, we need to run the \"event loop\" and go into the display thread by calling [`cv2.waitKey()`](http://docs.opencv.org/3.0-beta/modules/highgui/doc/user_interface.html?highlight=waitkey#waitkey). You can either give this function an integer argument, which is the amount of time to wait (in milliseconds) or leave out the argument, which means the display thread will wait forever until you press a key. The value returned is the ASCII code of the key you pressed. We can use this to wait for a specific key stroke and manipulate the image in different ways according to the key pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# press any key to close the window\n",
    "cv2.waitKey()\n",
    "# close the window\n",
    "cv2.destroyWindow('display')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Wait for a specific key, say 'x'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('dog.png', cv2.IMREAD_UNCHANGED)\n",
    "key = None\n",
    "while key != ord('x'):\n",
    "    cv2.imshow('display', image)\n",
    "    # wait for 30 ms\n",
    "    key = cv2.waitKey(30)\n",
    "# close the window\n",
    "cv2.destroyWindow('display')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** \n",
    "\n",
    "Do different things to the image according to the key pressed.\n",
    "- 'v': flip the image vertically\n",
    "- 'h': flip the image horizontally\n",
    "- 'u': upsample the image by a factor of 2\n",
    "- 'd': downsample the image by a factor of 2\n",
    "- 'r': show the red channel of the image\n",
    "- 'g': show the green channel of the image\n",
    "- 'b': show the blue channel of the image\n",
    "- 's': save the modified image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('dog.png', cv2.IMREAD_UNCHANGED)\n",
    "key = None\n",
    "while key != ord('x'):\n",
    "    cv2.imshow('original', image)\n",
    "    if key == ord('v'):\n",
    "        image_new = np.flipud(image)\n",
    "        cv2.imshow('modified', image_new)\n",
    "    elif key == ord('h'):\n",
    "        image_new = np.fliplr(image)\n",
    "        cv2.imshow('modified', image_new)\n",
    "    elif key == ord('u'):\n",
    "        # resize the input image to desired size\n",
    "        # 1st arg: image\n",
    "        # 2nd arg: desired size\n",
    "        # 3rd arg: interpolation method, default is bilinear\n",
    "        image_new = cv2.resize(image, \n",
    "                           dsize=(1024, 1024), \n",
    "                           interpolation=cv2.INTER_LINEAR)\n",
    "        cv2.imshow('modified', image_new)\n",
    "    elif key == ord('d'):\n",
    "        image_new = cv2.resize(image, \n",
    "                           (256, 256),\n",
    "                          interpolation=cv2.INTER_LINEAR)\n",
    "        cv2.imshow('modified', image_new)   \n",
    "    elif key == ord('r'):\n",
    "        image_new = image.copy()\n",
    "        image_new[..., [0,1]] = 0\n",
    "        cv2.imshow('modified', image_new)\n",
    "    elif key == ord('g'):\n",
    "        image_new = image.copy()\n",
    "        image_new[..., [0,2]] = 0\n",
    "        cv2.imshow('modified', image_new)\n",
    "    elif key == ord('b'):\n",
    "        image_new = image.copy()\n",
    "        image_new[..., [1,2]] = 0\n",
    "        cv2.imshow('modified', image_new)\n",
    "    elif key == ord('s'):\n",
    "        # save the image to a given path\n",
    "        # 1st arg: desired image path\n",
    "        # 2nd arg: image data\n",
    "        cv2.imwrite('dog_new.png', image_new)                \n",
    "    # wait for 30 ms\n",
    "    key = cv2.waitKey(30)\n",
    "# close all window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**useful functions**\n",
    "- [`cv2.imread()`](http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html?highlight=imread#cv2.imread)\n",
    "- [`cv2.imshow()`](http://docs.opencv.org/3.0-beta/modules/highgui/doc/user_interface.html?highlight=imshow#cv2.imshow)\n",
    "- [`cv2.imwrite()`](http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html?highlight=imwrite#cv2.imwrite)\n",
    "- [`cv2.waitKey()`](http://docs.opencv.org/3.0-beta/modules/highgui/doc/user_interface.html?highlight=waitkey#waitkey)\n",
    "- [`cv2.resize()`](http://docs.opencv.org/3.0-beta/modules/imgproc/doc/geometric_transformations.html?highlight=resize#cv2.resize)\n",
    "- More on [reading & writing images](http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/imgcodecs.html)\n",
    "- More on [geometric image transformations]( http://docs.opencv.org/3.0-beta/modules/imgproc/doc/geometric_transformations.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use webcam to capture a video\n",
    "The [`VideoCapture`](http://docs.opencv.org/3.0-beta/modules/videoio/doc/reading_and_writing_video.html?highlight=videocapture#videocapture) class provides funcationaly for video capturing from video files, image sequences or cameras. We can create a `VideoCapture` object by one of the following arguments:\n",
    "- a string pointing to a video file\n",
    "- an integer which is the device number of a camera\n",
    "- leave it empty and assign videos/cameras later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create `VideoCapture` object**\n",
    "\n",
    "Create a `VideoCapture` object and attach it to the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an object which can capture images.\n",
    "cap = cv2.VideoCapture()\n",
    "# Camera devices are indexed by integers, \n",
    "# typically your laptop webcam has device number 0.\n",
    "# Do a small loop to find the proper camera and open it.\n",
    "for i in range(10):\n",
    "    if cap.open(i):\n",
    "        print 'camera {} launched'.format(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read images**\n",
    "\n",
    "Once a `VideoCapture` object has been created and linked to videos/cameras, we can read images from it. By calling `VideoCapture::read()`, which returns a tuple of status and the actual image. A `False` status means failure in reading images. If `VideoCapture` successfully read in an image, we could display the image as usual using `cv2.imshow()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = None\n",
    "while key != ord('x'):\n",
    "    status, image = cap.read()\n",
    "    assert status, 'failed to read image from camera'\n",
    "    cv2.imshow('display', image)\n",
    "    key = cv2.waitKey(30)\n",
    "cv2.destroyWindow('display')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Release resources**\n",
    "\n",
    "Once done with the camera, we need to release the resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# release the camera\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Read and show a recorded video from the files ystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object with a filename pointing to a video.\n",
    "cap = cv2.VideoCapture('Megamind.avi')\n",
    "key = None\n",
    "while key != ord('x'):\n",
    "    status, image = cap.read()\n",
    "    if not status:\n",
    "        print 'end of video'\n",
    "        break\n",
    "    cv2.imshow('display', image)\n",
    "    key = cv2.waitKey(30)\n",
    "cv2.destroyWindow('display')\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code shows a minimal video player implemented in python with OpenCV. A lot of features are missing, such as pause, speed up/down the video, etc. One can do these by attach a track bar to the window. More information can be found in the [high-level gui (highgui) package](http://docs.opencv.org/3.0-beta/modules/highgui/doc/highgui.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Face detection in a video\n",
    "\n",
    "**<span style=\"color:red\">TODO: theory behind face detection ... and then the related functions.</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a cascade face detector.\n",
    "configure_file = 'haarcascade_frontalface_default.xml'\n",
    "face_detector = cv2.CascadeClassifier(configure_file)\n",
    "# let's load the lena image\n",
    "image = cv2.imread('lena.jpg')\n",
    "# The face detector takes a gray scale image as input,\n",
    "# so we need to convert the color image to grayscale image first.\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# Detect faces in multi-scale.\n",
    "# TODO: arguments explained ...\n",
    "faces = face_detector.detectMultiScale(gray, 1.1, 2)\n",
    "for (x, y, w, h) in faces:\n",
    "    image = cv2.rectangle(image, (x,y), (x+w, y+h), (255, 0, 0), 2)\n",
    "cv2.imshow('display', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyWindow('display')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions explained**\n",
    "\n",
    "- [`cv2.rectangle()`](http://docs.opencv.org/3.0-beta/modules/imgproc/doc/drawing_functions.html#rectangle): draw a rectangle on the given image (1st argument). Need to specify the top-left & bottom-right corners of the rectangle (2nd & 3rd argument, a tuple), RGB color of the rectangle border (4th argument) and width of border (5th argument).\n",
    "- Other useful drawing functions include\n",
    "[`cv2.circle()`](http://docs.opencv.org/3.0-beta/modules/imgproc/doc/drawing_functions.html#circle)(draw circles), \n",
    "[`cv2.line()`](http://docs.opencv.org/3.0-beta/modules/imgproc/doc/drawing_functions.html#line)(draw straight lines)\n",
    "and [`cv2.putText()`](http://docs.opencv.org/3.0-beta/modules/imgproc/doc/drawing_functions.html#puttext)(overlay texts on image).\n",
    "- More on [drawing functions](http://docs.opencv.org/3.0-beta/modules/imgproc/doc/drawing_functions.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Simplistic Face Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def overlay_image(foreground, background, rect):\n",
    "    \"\"\"\n",
    "    overlay the foreground image on the background image\n",
    "    at the specified location\n",
    "    :param foreground: the foreground image, numpy array\n",
    "    :param background: the background image, numpy array\n",
    "    :param rect: a rectangle indicating where to overlay\n",
    "    :return: overlaid image\n",
    "    \"\"\"\n",
    "    row, col = foreground.shape[0:2]\n",
    "    x, y, w, h = rect\n",
    "    xc, yc = x + w/2, y + h/2\n",
    "    ratio_x, ratio_y = w/float(col), h/float(row)\n",
    "    ratio = max(ratio_x, ratio_y)\n",
    "    row, col = int(ratio*row) & 0xfffe, int(ratio*col) & 0xfffe\n",
    "    resized = cv2.resize(foreground, dsize=(col, row))\n",
    "    ret = background.copy()\n",
    "    ymin = max(yc-row/2, 0)\n",
    "    ymax = min(yc+row/2, ret.shape[0])\n",
    "    xmin = max(xc-col/2, 0)\n",
    "    xmax = min(xc+col/2, ret.shape[1])\n",
    "    size_x, size_y = xmax-xmin, ymax-ymin\n",
    "    xo, yo = xmin-(xc-col/2), ymin-(yc-row/2)\n",
    "    # Copy resized foreground image to background.\n",
    "    ret[ymin:ymax, xmin:xmax, :] = resized[yo:size_y, xo:size_x, :]\n",
    "    return ret\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    face = cv2.imread('smiling_face.jpg')\n",
    "    face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    cap = cv2.VideoCapture()\n",
    "    for i in range(10):\n",
    "        if cap.open(i):\n",
    "            print 'camera {} launched'.format(i)\n",
    "            break\n",
    "    key = None\n",
    "    face_location = None\n",
    "    while key != ord('x'):\n",
    "        status, image = cap.read()\n",
    "        assert status, 'failed to grab image from camera'\n",
    "        # convert color image to grayscale image\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_detector.detectMultiScale(gray, 1.024, 10)\n",
    "        if len(faces) != 1:\n",
    "            # use the previous detection result\n",
    "            pass\n",
    "        else:\n",
    "            face_location = faces[0]\n",
    "        # Potential improvement: we can use filtering techniques (Kalman, particle, etc.) to \n",
    "        # smooth detection results, i.e., instead of throwing away previous detection results, \n",
    "        # we can re-use them along with the current detection \n",
    "        # and stablize the overall detection performance.\n",
    "        if face_location is not None:\n",
    "            image = overlay_image(foreground=face, background=image, rect=face_location)\n",
    "        cv2.imshow('display', image)\n",
    "        key = cv2.waitKey(30)\n",
    "    cv2.destroyWindow('display')\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature detection & matching/tracking\n",
    "Most SLAM (Simultaneously Localization and Mapping) systems rely on *sparse feature points*. A SLAM system takes a sequence of images and possibly measurements from other sensors, such as an IMU (Inertial Measurement Unit), as inputs and spits out the trajectory of the sensor platform as well as the 3D structure of the environment. The following video demonstrates the visual inertial SLAM system developled at UCLA Vision Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "from datetime import timedelta\n",
    "\n",
    "start=int(timedelta(hours=0, minutes=7, seconds=29).total_seconds())\n",
    "\n",
    "YouTubeVideo(\"H7mODetStyo\", start=start, width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical pipeline of a sparse feature based SLAM system includes: \n",
    "- Sparse feature detection: detect salient feature points in the image\n",
    "- Feature tracking/matching: establish correspondences between feature points across consecutive frames, either by tracking or matching\n",
    "- Camera pose estimation: compute the camera pose according to the feature correspondences\n",
    "- Triangulation: estimate depth of a feature point in 3D\n",
    "\n",
    "There are also some *direct* SLAM systems which don't rely on sparse feature points. To obtain the camera pose and depth estimation, they minimize the photometric error of *every possible pixel* instead of the small set of feature points which are carefully selected by the feature detector. \n",
    "\n",
    "The following video demonstrates an open source direct SLAM system, which is dubbed as [*LSD-SLAM*](https://github.com/tum-vision/lsd_slam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(\"GnuQzP3gty4\", width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on *Sparse feature detection* and *tracking/matching* today. You can find more about camera pose estimation & triangulation in the [book](http://vision.ucla.edu/MASKS/) and also the [OpenCV reference manual on 3D vision](http://docs.opencv.org/3.0-beta/modules/calib3d/doc/calib3d.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo**\n",
    "\n",
    "Feature matching for re-localization & failure recovery. The following video shows failure recovery behaviour of the real time visual inertial SLAM system developed at UCLA Vision Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(\"oQKnOHGkwTI\", width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful resources\n",
    "- [OpenCV API reference](http://docs.opencv.org/3.0-beta/index.html)\n",
    "- [OpenCV official repository](https://github.com/opencv/opencv)\n",
    "- [OpenCV code samples in Python](https://github.com/opencv/opencv/tree/master/samples/python)\n",
    "- [vlfeat tutorial](http://www.vlfeat.org/overview/tut.html)\n",
    "- [numpy tutorial](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html)\n",
    "- [Jupyter notebook](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
