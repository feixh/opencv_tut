{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Tutorial on OpenCV & MeshLab\n",
    "\n",
    "Xiaohan Fei\n",
    "\n",
    "\n",
    "`feixh@cs.ucla.edu`\n",
    "\n",
    "Github repository of this tutorial:\n",
    "\n",
    "https://github.com/feixh/opencv_tut\n",
    "\n",
    "The core OpenCV library is written in C++, but it has several language bindings, including python, java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV I/O functionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above piece of code imported all the packages we need through this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, display and save images\n",
    "Use the function [`cv2.imread()`](http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html?highlight=imread#cv2.imread) to read an image. The function accepts two arguments, the first argument is the path of the image and the second argument is a flag indicating how to decode the image, there are three options:\n",
    "- `cv2.IMREAD_COLOR`: load the image in color mode\n",
    "- `cv2.IMREAD_GRAYSCALE`: load the image in grayscale mode\n",
    "- `cv2.IMREAD_UNCHANGED`: load the image in color+alpha mode if the image has an alpha (controls transparency) channel, otherwise in color mode\n",
    "\n",
    "Use the function [`cv2.imshow()`](http://docs.opencv.org/3.0-beta/modules/highgui/doc/user_interface.html?highlight=imshow#cv2.imshow) to display an image. This function accepts two arguments, the first argument is the window name and the second one is the image, which is an numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the image\n",
    "image = cv2.imread('dog.png', cv2.IMREAD_COLOR)\n",
    "# TODO: check size of images loaded with different options\n",
    "print image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display the image\n",
    "cv2.imshow('display', image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above doesn't show us the image, we need to run the \"event loop\" and go into the display thread by calling [`cv2.waitKey()`](http://docs.opencv.org/3.0-beta/modules/highgui/doc/user_interface.html?highlight=waitkey#waitkey). You can either give this function an integer argument, which is the amount of time to wait (in milliseconds) or leave out the argument, which means the display thread will wait forever until you press a key. The value returned is the ASCII code of the key you pressed. We can use this to wait for a specific key stroke and manipulate the image in different ways according to the key pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# press any key to close the window\n",
    "cv2.waitKey()\n",
    "# close the window\n",
    "cv2.destroyWindow('display')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Wait for a specific key, say 'x'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('dog.png', cv2.IMREAD_UNCHANGED)\n",
    "key = None\n",
    "while key != ord('x'):\n",
    "    cv2.imshow('display', image)\n",
    "    # wait for 30 ms\n",
    "    key = cv2.waitKey(30)\n",
    "# close the window\n",
    "cv2.destroyWindow('display')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** \n",
    "\n",
    "Do different things to the image according to the key pressed.\n",
    "- 'v': flip the image vertically\n",
    "- 'h': flip the image horizontally\n",
    "- 'u': upsample the image by a factor of 2\n",
    "- 'd': downsample the image by a factor of 2\n",
    "- 'r': show the red channel of the image\n",
    "- 'g': show the green channel of the image\n",
    "- 'b': show the blue channel of the image\n",
    "- 's': save the modified image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('dog.png', cv2.IMREAD_UNCHANGED)\n",
    "key = None\n",
    "while key != ord('x'):\n",
    "    cv2.imshow('original', image)\n",
    "    if key == ord('v'):\n",
    "        image_new = np.flipud(image)\n",
    "        cv2.imshow('modified', image_new)\n",
    "    elif key == ord('h'):\n",
    "        image_new = np.fliplr(image)\n",
    "        cv2.imshow('modified', image_new)\n",
    "    elif key == ord('u'):\n",
    "        # resize the input image to desired size\n",
    "        # 1st arg: image\n",
    "        # 2nd arg: desired size\n",
    "        # 3rd arg: interpolation method, default is bilinear\n",
    "        image_new = cv2.resize(image, \n",
    "                           dsize=(1024, 1024), \n",
    "                           interpolation=cv2.INTER_LINEAR)\n",
    "        cv2.imshow('modified', image_new)\n",
    "    elif key == ord('d'):\n",
    "        image_new = cv2.resize(image, \n",
    "                           (256, 256),\n",
    "                          interpolation=cv2.INTER_LINEAR)\n",
    "        cv2.imshow('modified', image_new)   \n",
    "    elif key == ord('r'):\n",
    "        image_new = image.copy()\n",
    "        image_new[..., [0,1]] = 0\n",
    "        cv2.imshow('modified', image_new)\n",
    "    elif key == ord('g'):\n",
    "        image_new = image.copy()\n",
    "        image_new[..., [0,2]] = 0\n",
    "        cv2.imshow('modified', image_new)\n",
    "    elif key == ord('b'):\n",
    "        image_new = image.copy()\n",
    "        image_new[..., [1,2]] = 0\n",
    "        cv2.imshow('modified', image_new)\n",
    "    elif key == ord('s'):\n",
    "        # save the image to a given path\n",
    "        # 1st arg: desired image path\n",
    "        # 2nd arg: image data\n",
    "        cv2.imwrite('dog_new.png', image_new)                \n",
    "    # wait for 30 ms\n",
    "    key = cv2.waitKey(30)\n",
    "# close all window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**useful functions**\n",
    "- [`cv2.imread()`](http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html?highlight=imread#cv2.imread)\n",
    "- [`cv2.imshow()`](http://docs.opencv.org/3.0-beta/modules/highgui/doc/user_interface.html?highlight=imshow#cv2.imshow)\n",
    "- [`cv2.imwrite()`](http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html?highlight=imwrite#cv2.imwrite)\n",
    "- [`cv2.waitKey()`](http://docs.opencv.org/3.0-beta/modules/highgui/doc/user_interface.html?highlight=waitkey#waitkey)\n",
    "- [`cv2.resize()`](http://docs.opencv.org/3.0-beta/modules/imgproc/doc/geometric_transformations.html?highlight=resize#cv2.resize)\n",
    "- More on [reading & writing images](http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/imgcodecs.html)\n",
    "- More on [geometric image transformations]( http://docs.opencv.org/3.0-beta/modules/imgproc/doc/geometric_transformations.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use webcam to capture a video\n",
    "The [`VideoCapture`](http://docs.opencv.org/3.0-beta/modules/videoio/doc/reading_and_writing_video.html?highlight=videocapture#videocapture) class provides funcationaly for video capturing from video files, image sequences or cameras. We can create a `VideoCapture` object by one of the following arguments:\n",
    "- a string pointing to a video file\n",
    "- an integer which is the device number of a camera\n",
    "- leave it empty and assign videos/cameras later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create `VideoCapture` object**\n",
    "\n",
    "Create a `VideoCapture` object and attach it to the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an object which can capture images.\n",
    "cap = cv2.VideoCapture()\n",
    "# Camera devices are indexed by integers, \n",
    "# typically your laptop webcam has device number 0.\n",
    "# Do a small loop to find the proper camera and open it.\n",
    "for i in range(10):\n",
    "    if cap.open(i):\n",
    "        print 'camera {} launched'.format(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read images**\n",
    "\n",
    "Once a `VideoCapture` object has been created and linked to videos/cameras, we can read images from it. By calling `VideoCapture::read()`, which returns a tuple of status and the actual image. A `False` status means failure in reading images. If `VideoCapture` successfully read in an image, we could display the image as usual using `cv2.imshow()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = None\n",
    "while key != ord('x'):\n",
    "    status, image = cap.read()\n",
    "    assert status, 'failed to read image from camera'\n",
    "    cv2.imshow('display', image)\n",
    "    key = cv2.waitKey(30)\n",
    "cv2.destroyWindow('display')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Release resources**\n",
    "\n",
    "Once done with the camera, we need to release the resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# release the camera\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Read and show a recorded video from the files ystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an object with a filename pointing to a video.\n",
    "cap = cv2.VideoCapture('Megamind.avi')\n",
    "key = None\n",
    "while key != ord('x'):\n",
    "    status, image = cap.read()\n",
    "    if not status:\n",
    "        print 'end of video'\n",
    "        break\n",
    "    cv2.imshow('display', image)\n",
    "    key = cv2.waitKey(30)\n",
    "cv2.destroyWindow('display')\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code shows a minimal video player implemented in python with OpenCV. A lot of features are missing, such as pause, speed up/down the video, etc. One can do these by attach a track bar to the window. More information can be found in the [high-level gui (highgui) package](http://docs.opencv.org/3.0-beta/modules/highgui/doc/highgui.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Face detection in a video\n",
    "OpenCV provides a basic face classifier on top of which we can build a simple face detection app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Create a cascade face detector.\n",
    "configure_file = 'haarcascade_frontalface_default.xml'\n",
    "face_detector = cv2.CascadeClassifier(configure_file)\n",
    "# let's load the lena image\n",
    "image = cv2.imread('lena.jpg')\n",
    "\n",
    "# The face detector takes a gray scale image as input,\n",
    "# so we need to convert the color image to grayscale image first.\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# Detect faces in multi-scale.\n",
    "faces = face_detector.detectMultiScale(gray, 1.1, 2)\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image, (x,y), (x+w, y+h), (255, 0, 0), 2)\n",
    "cv2.imshow('display', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyWindow('display')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions explained**\n",
    "\n",
    "- [`cv2.rectangle()`](http://docs.opencv.org/3.0-beta/modules/imgproc/doc/drawing_functions.html#rectangle): draw a rectangle on the given image (1st argument). Need to specify the top-left & bottom-right corners of the rectangle (2nd & 3rd argument, a tuple), RGB color of the rectangle border (4th argument) and width of border (5th argument).\n",
    "- Other useful drawing functions include\n",
    "[`cv2.circle()`](http://docs.opencv.org/3.0-beta/modules/imgproc/doc/drawing_functions.html#circle)(draw circles), \n",
    "[`cv2.line()`](http://docs.opencv.org/3.0-beta/modules/imgproc/doc/drawing_functions.html#line)(draw straight lines)\n",
    "and [`cv2.putText()`](http://docs.opencv.org/3.0-beta/modules/imgproc/doc/drawing_functions.html#puttext)(overlay texts on image).\n",
    "- More on [drawing functions](http://docs.opencv.org/3.0-beta/modules/imgproc/doc/drawing_functions.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Simplistic Face Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def overlay_image(foreground, background, rect):\n",
    "    \"\"\"\n",
    "    overlay the foreground image on the background image\n",
    "    at the specified location\n",
    "    :param foreground: the foreground image, numpy array\n",
    "    :param background: the background image, numpy array\n",
    "    :param rect: a rectangle indicating where to overlay\n",
    "    :return: overlaid image\n",
    "    \"\"\"\n",
    "    row, col = foreground.shape[0:2]\n",
    "    x, y, w, h = rect\n",
    "    xc, yc = x + w/2, y + h/2\n",
    "    ratio_x, ratio_y = w/float(col), h/float(row)\n",
    "    ratio = max(ratio_x, ratio_y)\n",
    "    row, col = int(ratio*row) & 0xfffe, int(ratio*col) & 0xfffe\n",
    "    resized = cv2.resize(foreground, dsize=(col, row))\n",
    "    ret = background.copy()\n",
    "    ymin = max(yc-row/2, 0)\n",
    "    ymax = min(yc+row/2, ret.shape[0])\n",
    "    xmin = max(xc-col/2, 0)\n",
    "    xmax = min(xc+col/2, ret.shape[1])\n",
    "    size_x, size_y = xmax-xmin, ymax-ymin\n",
    "    xo, yo = xmin-(xc-col/2), ymin-(yc-row/2)\n",
    "    # Copy resized foreground image to background.\n",
    "    ret[ymin:ymax, xmin:xmax, :] = resized[yo:size_y, xo:size_x, :]\n",
    "    return ret\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    face = cv2.imread('smiling_face.jpg')\n",
    "    face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    cap = cv2.VideoCapture()\n",
    "    for i in range(10):\n",
    "        if cap.open(i):\n",
    "            print 'camera {} launched'.format(i)\n",
    "            break\n",
    "    key = None\n",
    "    face_location = None\n",
    "    while key != ord('x'):\n",
    "        status, image = cap.read()\n",
    "        assert status, 'failed to grab image from camera'\n",
    "        # convert color image to grayscale image\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_detector.detectMultiScale(gray, 1.2, 4)\n",
    "        if len(faces) != 1:\n",
    "            # use the previous detection result\n",
    "            pass\n",
    "        else:\n",
    "            face_location = faces[0]\n",
    "        # Potential improvement: we can use filtering techniques (Kalman, particle, etc.) to \n",
    "        # smooth detection results, i.e., instead of throwing away previous detection results, \n",
    "        # we can re-use them along with the current detection \n",
    "        # and stablize the overall detection performance.\n",
    "        if face_location is not None:\n",
    "            image = overlay_image(foreground=face, background=image, rect=face_location)\n",
    "        cv2.imshow('display', image)\n",
    "        key = cv2.waitKey(30)\n",
    "    cv2.destroyWindow('display')\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature detection & matching/tracking\n",
    "Most SLAM (Simultaneously Localization and Mapping) systems rely on *sparse feature points*. A SLAM system takes a sequence of images and possibly measurements from other sensors, such as an IMU (Inertial Measurement Unit), as inputs and spits out the trajectory of the sensor platform as well as the 3D structure of the environment. The following video demonstrates the visual inertial SLAM system developled at UCLA Vision Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "from datetime import timedelta\n",
    "\n",
    "start=int(timedelta(hours=0, minutes=7, seconds=29).total_seconds())\n",
    "\n",
    "YouTubeVideo(\"H7mODetStyo\", start=start, width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some *direct* SLAM systems which don't rely on sparse feature points. To obtain the camera pose and depth estimation, they minimize the photometric error of *every possible pixel* instead of the small set of feature points which are carefully selected by the feature detector. \n",
    "\n",
    "The following video demonstrates an open source direct SLAM system, which is dubbed as [*LSD-SLAM*](https://github.com/tum-vision/lsd_slam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"GnuQzP3gty4\", width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will focus on sparse feature based SLAM. \n",
    "\n",
    "Typical pipeline of a sparse feature based SLAM system includes: \n",
    "- Sparse feature detection: detect salient feature points in the image\n",
    "- Feature tracking/matching: establish correspondences between feature points across consecutive frames, either by tracking or matching\n",
    "- Camera pose estimation: compute the camera pose according to the feature correspondences\n",
    "- Triangulation: estimate depth of a feature point in 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on *Sparse feature detection* and *tracking/matching* today. You can find more about camera pose estimation & triangulation in the [book](http://vision.ucla.edu/MASKS/) and also the [OpenCV reference manual on 3D vision](http://docs.opencv.org/3.0-beta/modules/calib3d/doc/calib3d.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature**\n",
    "\n",
    "Features are image regions which are discriminative enough to be found. They can be points, lines or regions. In the image below, if you look at the homogeneous regions, it's hard to distinguish one region from the other. If you look at the texture-rich regions, such as the texts or the picture on the box, there are more details on which you can depend to distinguish them.\n",
    "\n",
    "![](box.png)\n",
    "\n",
    "OpenCV provides a very rich set of functions to detect different kinds of features, such as \n",
    "[FAST](http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_fast/py_fast.html), \n",
    "[Harris](http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html), \n",
    "[SIFT](http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html), etc. Today we will focus on FAST corner detector, which is very popular in robotic applications.\n",
    "\n",
    "Essentially, FAST solves a binary classification problem: given a pixel, it tells us whether that pixel is a corner or not. The basic idea is to test whether a small set of pixels sampled around the target pixel follows a specific pattern. The pattern is learned from a training set offline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Fast corner detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the box image\n",
    "img = cv2.imread('box.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Create a FAST corner detector with default parameters,\n",
    "# the interface is consistent across all the detectors, just replace 'FAST'.\n",
    "detector = cv2.FeatureDetector_create('FAST')\n",
    "\n",
    "# find and draw the keypoints\n",
    "kp1 = detector.detect(img)\n",
    "img1 = cv2.drawKeypoints(img, kp1, color=(255,0,0))\n",
    "\n",
    "# visualize\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.axis('off')\n",
    "plt.title('thresh={}; non-max={}; #pts={}'.format(\n",
    "    detector.getInt('threshold'), \n",
    "    detector.getBool('nonmaxSuppression'), \n",
    "    len(kp1)))\n",
    "\n",
    "# print parameters\n",
    "print 'threshold=', detector.getInt('threshold')\n",
    "print 'non-maximum suppression', detector.getBool('nonmaxSuppression')\n",
    "\n",
    "# set parameters\n",
    "# detector.setBool('nonmaxSuppression', False)\n",
    "detector.setInt('threshold', 50)\n",
    "kp2 = detector.detect(img)\n",
    "img2 = cv2.drawKeypoints(img, kp2, color=(0, 255, 0))\n",
    "\n",
    "# visualize\n",
    "plt.subplot(122)\n",
    "plt.imshow(img2)\n",
    "plt.axis('off')\n",
    "plt.title('thresh={}; non-max={}, #pts={}'.format(\n",
    "    detector.getInt('threshold'), \n",
    "    detector.getBool('nonmaxSuppression'), \n",
    "    len(kp2)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OpenCV, the results of various feature detection algorithms are represented as lists of `KeyPoint`. Here, we investigate the `KeyPoint` class a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print '#detected features=', len(kp1)\n",
    "print 'type of keypoint=', type(kp1[0])\n",
    "print 'response of feature=', kp1[0].response\n",
    "print 'position of feature=', kp1[0].pt\n",
    "\n",
    "print 'size of feature=', kp1[0].size\n",
    "print 'octave of feature=', kp1[0].octave\n",
    "print 'angle of feature=', kp1[0].angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KeyPoint::pt` is the position of the feature; `KeyPoint::response` denotes how salient the feature is; other fields including `size`, `octave` and `angle` are useful in SIFT, ORB and other scale-space based and orientation robust detectors, but not used in FAST.\n",
    "\n",
    "The field `KeyPoint::response` is very useful if you want to pick features based on their quality. You can sort the features based on their responses and pick the top ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correspondence**\n",
    "\n",
    "Once features are detected, we want to establish correspondences between different images, so that the camera pose and depth can be computed further. There are two different techniques to establish correspondences, tracking and matching. If two images are slightly different, for instance two consecutive frames in a video, we can use *optical flow* algorithm to track the features.\n",
    "\n",
    "video frame 1     |  video frame 2\n",
    ":-------------------:|:------------------:\n",
    "![](frame1.png)  |  ![](frame2.png)\n",
    "\n",
    "\n",
    "\n",
    "If two images roughly contain the same stuff but from different viewpoints, we can attribute a descriptor, which is like a signature, to each feature and match the features by looking for nearest neighbors in the descriptor space.\n",
    "\n",
    "box       |  box in the scene\n",
    ":-------------------:|:------------------:\n",
    "![](box.png)  |  ![](box_in_scene.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_video/py_lucas_kanade/py_lucas_kanade.html) is a nice tutorial on optical flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Create some random colors\n",
    "color = np.random.randint(0, 255, (100,3))\n",
    "\n",
    "# Take first frame and find corners in it\n",
    "frame1 = cv2.imread('frame1.png', cv2.IMREAD_GRAYSCALE)\n",
    "frame2 = cv2.imread('frame2.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Setup the detector\n",
    "detector = cv2.FeatureDetector_create('FAST')\n",
    "detector.setInt('threshold', 20)\n",
    "detector.setBool('nonmaxSuppression', True)\n",
    "\n",
    "# Detect a couple of FAST corner features\n",
    "kps = detector.detect(frame1)\n",
    "p0 = np.zeros((len(kps), 1, 2), dtype='f')\n",
    "for i, kp in enumerate(kps):\n",
    "    p0[i, 0, :] = kp.pt    \n",
    "    \n",
    "# compute optical flow\n",
    "p1, st, err = cv2.calcOpticalFlowPyrLK(frame1, frame2, p0, None, **lk_params)\n",
    "\n",
    "# Select good points\n",
    "good_new = p1[st==1]\n",
    "good_old = p0[st==1]\n",
    "\n",
    "# draw the tracks\n",
    "display = cv2.cvtColor(frame2, cv2.COLOR_GRAY2RGB)\n",
    "for i,(new,old) in enumerate(zip(good_new,good_old)):\n",
    "    a,b = new.ravel()\n",
    "    c,d = old.ravel()\n",
    "\n",
    "    cv2.line(display, (a,b), (c,d), color[i % 100].tolist(), 2)\n",
    "\n",
    "\n",
    "plt.imshow(display)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Track a cereal box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 4,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Setup the detector\n",
    "detector = cv2.FeatureDetector_create('FAST')\n",
    "detector.setInt('threshold', 10)\n",
    "detector.setBool('nonmaxSuppression', True)\n",
    "\n",
    "frame1 = None\n",
    "frame2 = None\n",
    "key = None\n",
    "plt.ion()\n",
    "while key != ord('x'):\n",
    "    if frame1 is None:\n",
    "        status, frame1 = cap.read()\n",
    "        frame1 = cv2.cvtColor(frame1, cv2.COLOR_RGB2GRAY)\n",
    "        # Detect a couple of FAST corner features\n",
    "        kps = detector.detect(frame1)\n",
    "        p0 = np.zeros((len(kps), 1, 2), dtype='f')\n",
    "        for i, kp in enumerate(kps):\n",
    "            p0[i, 0, :] = kp.pt    \n",
    "    else:\n",
    "        status, frame2 = cap.read()\n",
    "        frame2 = cv2.cvtColor(frame2, cv2.COLOR_RGB2GRAY)            \n",
    "        # compute optical flow\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(frame1, frame2, p0, None, **lk_params)\n",
    "        # Select good points\n",
    "        good_new = p1[st==1]\n",
    "        good_old = p0[st==1]\n",
    "\n",
    "        # draw the tracks\n",
    "        display = cv2.cvtColor(frame2, cv2.COLOR_GRAY2RGB)\n",
    "        for i,(new, old) in enumerate(zip(good_new,good_old)):\n",
    "            a,b = new.ravel()\n",
    "            c,d = old.ravel()\n",
    "            cv2.line(display, (a,b), (c,d), (0, 255, 0), 2)\n",
    "            \n",
    "        frame1 = frame2.copy()\n",
    "        p0 = good_new.reshape(-1,1,2)\n",
    "        cv2.imshow('flow', display)\n",
    "        key = cv2.waitKey(30)\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** \n",
    "\n",
    "BRIEF descriptor and matching.\n",
    "\n",
    "BRIEF is a binary descriptor: it's represented as a binary string. A typical BRIEF descriptor contains 32 bytes, which is 256 bits. Since they are binary, comparison between BRIEF descriptors can be done with bitwise operations, which are well supported by modern computers and thus are very fast. \n",
    "\n",
    "The construction of BRIEF descriptors: randomly sample pairs of pixels in the neighborhood of a feature point, compare the intensities of each pixel pair and mark down the comparison result with 1/0. Since the whole procedure only involves pixel comparisons, it's also very fast.\n",
    "\n",
    "Because of its efficiency, BRIEF is very popular in real time SLAM systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load images\n",
    "img1 = cv2.imread('box.png', cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('box_in_scene.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Create the feature detector\n",
    "fast = cv2.FeatureDetector_create('FAST')\n",
    "fast.setInt('threshold', 20)\n",
    "fast.setBool('nonmaxSuppression', True)\n",
    "\n",
    "# Detect FAST features\n",
    "kp1 = fast.detect(img1, None)\n",
    "kp2 = fast.detect(img2, None)\n",
    "\n",
    "# Create the descriptor extractor\n",
    "brief = cv2.DescriptorExtractor_create('BRIEF')\n",
    "\n",
    "# Extract BRIEF descriptors\n",
    "kp1, desc1 = brief.compute(img1, kp1)\n",
    "kp2, desc2 = brief.compute(img2, kp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptor extractor returns the descriptors along with the corresponding keypoints. Descritpors are arranged in the form of numpy array with each row representing one descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'type of descriptors=', type(desc1)\n",
    "print 'shape of descriptors=', desc1.shape\n",
    "print 'type of descriptor=', desc1[0].dtype\n",
    "print 'content of descriptor=', desc1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper import drawMatches\n",
    "\n",
    "# create BFMatcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Match descriptors.\n",
    "raw_matches = bf.match(desc1,desc2)\n",
    "\n",
    "# Sort them in the order of their distance.\n",
    "raw_matches = sorted(raw_matches, key = lambda x:x.distance)\n",
    "\n",
    "# Threshold on the distance\n",
    "# threshold = 32\n",
    "# matches = [x for x in raw_matches if x.distance < threshold]\n",
    "matches = raw_matches[0:50]\n",
    "\n",
    "\n",
    "# Draw \n",
    "img3 = drawMatches(img1, kp1, img2, kp2, matches)\n",
    "\n",
    "plt.imshow(img3)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print '#matches', len(matches)\n",
    "print 'type of matching result', type(matches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching results are not good because BRIEF descriptor is not invariant to scale change. So even for the same point in physical space, the descriptors computed from different images can be very different, thus it's very hard to find the right correspondences by nearest neighbor search in descriptor space. We can use SIFT descriptor instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "SIFT feature detection and matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import drawMatches\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread('box.png', cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('box_in_scene.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, desc1 = sift.detectAndCompute(img1, None)\n",
    "kp2, desc2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "# create BFMatcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "# Match descriptors.\n",
    "raw_matches = bf.match(desc1, desc2)\n",
    "\n",
    "# Sort them in the order of their distance.\n",
    "raw_matches = sorted(raw_matches, key = lambda x:x.distance)\n",
    "\n",
    "# # Threshold on the distance\n",
    "# threshold = 200\n",
    "# matches = [x for x in raw_matches if x.distance < threshold]\n",
    "matches = raw_matches[0:50]\n",
    "\n",
    "# Draw \n",
    "img3 = drawMatches(img1, kp1, img2, kp2, matches)\n",
    "\n",
    "plt.imshow(img3)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching results are much better, but still some matches are obviously wrong. We call these wrong matches *outliers* while the correct matches are *inliers*. A widely used approach to find and remove outliers is called *RANSAC*. The basic idea is as follows:\n",
    "1. Assume all the inlier data points are produced by a model with unknown parameters.\n",
    "- Randomly sample some data points from which model parameters are estimated.\n",
    "- Check each data point against the estimated model and count how many of them follow the estimated model.\n",
    "- Repeat step 2 & 3 several times and pick the best model. Along with the best model, we also have a set of data points which follow the model well (inliers), and the rest are outliers.\n",
    "\n",
    "A detailed example on applying RANSAC to feature matching can be found [here](http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo**\n",
    "\n",
    "Feature matching for re-localization & failure recovery. The following video shows failure recovery behaviour of the real time visual inertial SLAM system developed at UCLA Vision Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"oQKnOHGkwTI\", width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful resources\n",
    "- [OpenCV API reference](http://docs.opencv.org/3.0-beta/index.html)\n",
    "- [OpenCV Python tutorials](http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html)\n",
    "- [OpenCV official repository](https://github.com/opencv/opencv)\n",
    "- [OpenCV code samples in Python](https://github.com/opencv/opencv/tree/master/samples/python)\n",
    "- [vlfeat tutorial](http://www.vlfeat.org/overview/tut.html)\n",
    "- [numpy tutorial](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html)\n",
    "- [Jupyter notebook](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
